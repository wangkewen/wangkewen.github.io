<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CSC8530</title>
<link rel="stylesheet" href="style.css" type="text/css">
</head>
<body>
<div id="content">
<div id="header">
<span id="course"> CSC8530 </span>
<span id="sign">Kewen Wang</span>
</div>
<div id="leftcontent"> 
<a href="index.html"><img id="logo" src="MapReduce.png" alt="LOGO"></a>
<div id="taskslist">Tasks List</div>
<div id="nav">
<ul id="list">
<li class="task"><a class="tasklink" href="index.html">Topic Selection</a></li>
<li class="task"><a class="tasklink" href="paperlist.html">Bibliography of literature found</a></li>
<li class="task"><a class="tasklink" href="comment.html">Annotated Bibliography of the literature found</a></li>
<li class="task"><a class="tasklink" href="result.html">Detailed Annotated Bibliography and Classification of the Results</a></li>
<li class="task"><a class="tasklink" href="survey.html">Survey Paper</a></li>
</ul>
</div>
<div id="contact">Contact</div>
<div id="email">kwang12@student.gsu.edu</div>
</div>
<div id="rightcontent">
<h1>Classification</h1>
<h2>MapReduce Architecture</h2>
<ul id="paperlist">
<li class="paper"><a href="http://idning-ebook.googlecode.com/svn/trunk/google/%23mapreduce-osdi04.pdf">MapReduce: Simplified data processing on large clusters. <i>(2004)</i></a>
<div class="comments">In this paper, MapReduce programming model is presented from Google Inc. This computation model takes key/value pairs as input data and generates key/value pairs as output files. The two main functions of this model are map and reduce. Map function gets and processes the input data, and transfer the non-structural input data into key/value pairs. Reduce function merges the output data of map function and produce final output data in the format of key/value pairs. Many interesting programs could be implemented by applying MapReduce: sorting, visiting frequency analysis in website, graph processing.
<br>For MapReduce computation model, it follows master-worker structures for MapReduce job execution. Master machine keeps the status and identity of worker machines, and is responsible for assigning map/reduce tasks to worker machines. 
<br>In the distributed environment, the input data of MapReduce job is split into M pieces, and then the copies of MapReduce program is sent to master and worker machines. Master machine will assign M map tasks to workers, and R reduce tasks to other workers. Workers of map tasks read the corresponding input split and generate key/value pairs according map function. These output pairs in the buffer will be periodically written to local disk. The location information of these pairs will be passed to master. Notified by the master, workers of reduce task use remote procedure call to read output of map workers and sort them so that pairs with same key are grouped together. The corresponding intermediate values will be sent to reduce function, and produce final output for this reduce partition. When all the map and reduce tasks are completed, output will return to the user program.
<br>MapReduce model has the fault tolerance mechanism when workers or master fails, and it has locality for data reading through 3 copies of input data. In addition, the master runs a HTTP server and maintains status pages for running MapReduce job. This status page provides data flow information and job execution time information.
</div></li>
<li class="paper"><a href="http://www.cs.arizona.edu/~bkmoon/papers/sigmodrec11.pdf">Parallel data processing with MapReduce: a survey. <i>(2012)</i></a> 
<div class="comments">It is a survey paper about MapReduce, it uses Hadoop as an example to compare with DBMS to summarize the advantages and disadvantages of MapReduce. 
<br>It analyzes five advantages of MapReduce. It contains only two main functions map and reduce and physical distribution of the MapReduce is not required, so it is simple and easy to use; it is flexible to process irregular or unstructured data compared to DBMS because it does not depend on data model; MapReduce is independent of the storage layers; it provides fault tolerance to ensure continue work when some working machines fail; MapReduce could support high scalability that Hadoop could scale out to support more than 4000 nodes.
<br>But MapReduce has five disadvantages compared to DBMS. MapReduce does not provide direct support for high-level languages like SQL, which is supported by DBMS; MapReduce has no schema and no index, so it needs to parse each element of the input data; it is a single fixed dataflow, some algorithms are not supported well by using MapReduce; pipeline parallelism is not exploited in MapReduce, and MapReduce does not have execution plans which are often optimized in DBMS to reduce data transmission; MapReduce is too young and not mature compared to 40 years old DBMS, and few third-party tools are available to support. Moreover, this paper presents some approaches to improve the MapReduce framework: some tools like Pig and Hive could provide some high-level languages like SQL in MapReduce; data compression could solve data size problem; mapreduce model could be modified to support iteration programs or pipelined processing; using index structures to reduce I/O cost; schedule scheme could be carefully designed to improve execution time; tuning system parameters to improve MapReduce job performance.
</div></li>
</ul>
<h2>Optimization Approaches</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/pdf/1302.2966.pdf">The Family of MapReduce and Large Scale Data Processing Systems. <i>(2013)</i></a>
<div class="comments">In this paper, some techniques are proposed to improve the performance of MapReduce programs from many perspectives and many data processing systems are designed and implemented on MapReduce.
<br>These possible improvements are based on the limitations of the basic MapReduce architecture. One big limitation of MapReduce is there is no direct support for operation joining of multiple datasets, an approach to optimize the communication cost of join operation could improve corresponding mapreduce programs; some systems like HaLoop could support iterative processing for MapReduce in some algorithms; data and process sharing in MRShare could reduce data transmission cost in MapReduce; another approach is to add data index to reduce query cost; systems like CoHadoop provide mechanism to control data placement to achieve good load balance for MapReduce job; systems like Incoop supports incremental computations to support pipeling and streaming operations; and analyzing MapReduce programs to detect opportunities for relational style optimizations. 
<br>MapReduce has no SQL-like support, some efforts have been proposed to provide SQL interfaces on the top of MapReduce. Sawzall is script language on top of MapReduce; Pig Latin provides SQL query model above MapReduce programs; Hive supports queries in SQL; Tenzing is SQL query execution engine; SQL/MR allows custom defined functions of programming language and improve SQL functionality; HadoopDB is a hybrid system to combine scalable feature in MapReduce and some advantages in parallel databases; and Jaql is a kind of query language designed for Javascript Object Notation (JSON). Moreover, this paper introduces some systems that are similar to basic ideas of MapReduce framework: SCOPE, Dryad/DryadLinq, Spark, Nephle/Pact, Boom Analytics, Hyracks/ ASTERIX.
</div></li>
<li class="paper"><a href="https://lambda.uta.edu/mrql.pdf">An optimization framework for map-reduce queries. <i>(2012)</i></a>
<div class="comments">In this paper, Map-Reduce Query Language (MRQL) is discussed for MapReduce computations. Some method presents an optimization of this kind of queries into MapReduce jobs. This paper gives the details of MRQL queries to MapReduce translation. 
<br>MRQL can support input data of JSON, XML, binary and record text documents, and its semantics has been inspired by the functional programings. This framework contains some MR operators that capture the MRQL functionality. The MRQL will be firstly translated into algebraic form and then translate to a physical plan of MR operators.
<br>Besides, this paper presents an optimization framework to provide a good plan of physical operators. It includes 5 steps: simplifing the query by eliminating some query nesting, constructing the query graph of the execution plan, deriving an algebraic form from the query graph, algebraic optimization to minimize the number of MR jobs and to avoid redundant operations, synthesizing the combine function.
</div></li>
<li class="paper"><a href="http://www.infosun.fim.uni-passau.de/cl/publications/docs/DAL13mapreduce.pdf">Modeling and Optimizing MapReduce Programs. <i>(2013)</i></a>
<div class="comments">It is a technical report provides the detailed analysis of the functional model in MapReduce framework, and discusses Map, Shuffle and Reduce, which are the main phases of MapReduce job. 
<br>It uses functional language Haskell to represent the functional model of MapReduce, and model the data flow by means of steps of Mapper and Reducer tasks. Mapper tasks are executed on many cluster nodes, and user-defined mapper function is run in distributed. Shuffle execution is in the middle between mapper and reducer. Reducer is described in the flow of sort, group and reducer.
<br>In addition, it designs a cost model for MapReduce jobs to measure its performance from the aspect of execution time. It constructs the equations to calculate the time cost of a MapReduce job execution based on the input data information and parameters. Based on the MapReduce functional model and cost model, it provides rules for optimization and demonstrates the effectiveness.
</div></li>
<li class="paper"><a href="http://cloud.pubs.dbs.uni-leipzig.de/sites/cloud.pubs.dbs.uni-leipzig.de/files/Jiang2010TheperformanceofmapreduceAnindepthstudy.pdf">The Performance of MapReduce: An In-depth Study. <i>(2010)</i></a>
<div class="comments">This paper discusses some factors that affect the performance of MapReduce. MapReduce programming model is an important factor that impacts its performance. And MapReduce is independent of storage, there are some factors: I/O mode, the way of fetching data from storage system; data parsing, the scheme how reader parse the format of the record; indexing, the methods to utilize indexes to speed up data processing; scheduling, the scheme to assign map and reduce tasks execution through scheduler.
<br>And these factors could be combined to improve the MapReduce performance, this paper applies experiments on benchmark of different kinds of tasks to prove the effect through adjusting these factors.
</div></li>
</ul>
<h2>Improved MapReduce System</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/ftp/arxiv/papers/1104/1104.3217.pdf">Automatic optimization for MapReduce programs. <i>(2011)</i></a>
<div class="comments">In this paper, a system MANIMAL is designed to search for optimizing chances in MapReduce programs. It provides an optimization method without modifying any MapReduce codes. 
<br>MANIMAL targets three kinds of optimization: selections that could be optimized by using B+ tree to scan only the relevant faction of the input data; projection that optimized by storing only bytes necessary for executing the MapReduce codes, this optimization is similar to column-store; data compression could reduce data size in storing and transferring during MapReduce execution, MANIMAL enables delta-compression and direct-compression.
<br>The MANIMAL contains three main parts: analyzer, optimizer and execution fabric. Analyzer could provide optimization descriptor for optimizer after viewing and checking programs in MapReduce. Optimizer will analyze optimization choices and form a real executable plan for execution fabric. And execution fabric will run MapReduce programs according to selected plan. The analyzer will examine the MapReduce programs to find optimization opportunities, so it is the foundation of MANIMAL system. Analyzer applies control flow graph (CFG) to analyze all possible paths the program may take, and uses dataflow analysis to compute reaching definitions.
</div></li>
<li class="paper"><a href="http://static.usenix.org/events/nsdi10/tech/full_papers/condie.pdf">MapReduce Online. <i>(2010)</i></a>
<div class="comments">In this paper, Hadoop Online is provided to add the pipelining support on Hadoop which is designed based on MapReduce [Condie et al. 2010]. It could enable the pipelining function among continuing tasks and sequential jobs. 
<br>Hadoop Online supports pipelining within a job through modifying the mapreduce codes to enable mapper pushing data to reducers instead of pulling data by reducers, and pipeline stalls could free map tasks after completing the tasks. And it could pipeline the reduce output of one job directly to the map tasks of the next job to support pipelining between jobs. Besides, it supports single job online aggregation and multi jobs online aggregation.
<br>Moreover, the pipelined Hadoop allows MapReduce jobs run continuously accepting new data when it is available and processing it immediately. For map tasks, output data is already sent to reduce tasks once it is generated; for reduce tasks, reduce function need to be invoked periodically when map output available at the reducers.
</div></li>
<li class="paper"><a href="http://pdf.aminer.org/000/225/039/a_practical_approach_to_static_node_positioning.pdf">SQL/MapReduce: A practical approach to self-describing, polymorphic, and parallelizable user-defined functions. <i>(2009)</i></a>
<div class="comments">In this paper, SQL/MapReduce (SQL/MR) is designed to implement User-Defined Functions. It can process computation for parallel database systems. Its syntax is near to SQL query. It requires ON clause to specify the input to the SQL/MR function, and the PARTITION BY clause is used to partition the input. And ORDER BY clause will be used to sort the data. Besides, user can add some custom argument clauses. It contains programming interfaces: runtime contract, processing data functions, combiner functions and running aggregates. The programming model of SQL/MR functions is general to MapReduce. SQL/MR is integrated in nCluster, which is a shared-nothing parallel database. The implementation of SQL/MR in nCluster needs define the interaction of SQL/MR function with query planning and query execution framework.
</div></li>
<li class="paper"><a href="http://pasa-bigdata.nju.edu.cn/people/ronggu/pub/SHadoop_JPDC.pdf">SHadoop: Improving MapReduce performance by optimizing job execution mechanism in Hadoop clusters. <i>(2014)</i></a>
<div class="comments">This paper proposes SHadoop to improve the basic Hadoop system to reduce job execution time for MapReduce jobs. SHadoop analyzes the setup and cleanup phases of MapReduce job to reduce the execution time of startup and cleanup and provides a more efficient method for event notification
<br>In MapReduce job setup/cleanup, SHadoop execute the job setup and cleanup task on the JobTracker side instead of sending messages to TaskTracker to launch the job setup and cleanup task by periodical heartbeats in original Hadoop. It will avoid four heartbeats in the standard Hadoop.
<br>For notification mechanism, SHadoop separates the critical event messages from heartbeat messages and design a new instant messaging communication mechanism for critical message notification so critical event (like task completion event) message will be sent to the JobTracker immediately.
</div></li>
</ul>
<h2>SQL in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-7.pdf">Ysmart: Yet another sql-to-mapreduce translator. <i>(2011)</i></a>
<div class="comments">In this paper, a system YSmart is designed to implement the SQL queries in MapReduce programs, and it can find the optimization chances among complex queries.
<br>Auto-generated MapReduce may have too long time execution time in some critical operations, and may create unnecessary jobs. The bottleneck of translating SQL to MapReduce is that they cannot solve the limitations of MapReduce programming framework for complex queries and have not mechanism to search for optimization opportunities in nesting or intra queries.
<br>YSmart is designed to optimize complex query without changing MapReduce framework. It firstly detects intra-query correlations in a query, and then uses some rules to generate optimized MapReduce jobs through Common MapReduce Framework (CMF). YSmart aims at minimal number of jobs to finish some correlated query operations. For selection and projection operation, it is straightforward. For aggregation with grouping, it can be finished in the reduce phase. For joining of two datasets, each data is partitioned by its columns and join is finished in reduce phase.
<br>SQL to MapReduce translators use one mapreduce job for one operation mode of database systems. For DBMS, it uses a pipelined and iterative interconnection among many operators. But MapReduce is different from DBMS, so one-operation-to-one-job mode is not proper, and another mode many operations in one MapReduce job of executing many operations in a single job would be a more efficient choice than one-to-one translation.
<br>YSmart detect three correlations: input correlation, transit correlation and job flow correlation. For input correction, the corresponding jobs can share the table scan in map phase; for transit correlation, there are some overlapped data between map outputs; for job flow correlation, the node can be evaluated in the reduce phase. Common MapReduce Framework (CMF) is important component of YSmart, it is a flexible framework that allows different types of MapReduce jobs and merges multiple jobs in a common job with low cost. The common mapper executes selection or projection operations and common reducer executes join or aggreration operations, and post-job computation could execute further computation for the output of previous jobs.
</div></li>
<li class="paper"><a href="http://www.chinacloud.cn/upload/2013-10/13100807567165.pdf">JackHare: a framework for SQL to NoSQL translation using MapReduce. <i>(2013)</i></a>
<div class="comments">In this paper, JackHare is designed as a programming framework for SQL to MapReduce translation large scale data analysis stored from HBase. JackHare framework includes SQL query compiler and uses JDBC driver to process unstructured data in HBase as datastore.
<br>The process of operations in JackHare is: submit ANSI-SQL query through SQL client application, and compiler scans and parses the query, look up the table of HBase and generate MapReduce programs according to query commands, access HBase and execute MapReduce job, the results will be returned to SQL client application based on RDB schema.
<br>The method of implementing nested SQL queries and complicated SQL in MapReduce is explained. For select statement, map phase is sufficient to get query result and reduce is not necessary; for where clause, filter instance can be customized to filter input table; for grouping query, the mappers select the value of the column specified by the query as the key, and the reducers collect the key/value pairs with the same key; for aggregate query, the aggregation function is implemented in the reduce phase; for joining query, another job may be needed; for sorting query, the rows of the table are sorted by the row key in HBase.
</div></li>
<li class="paper"><a href="http://conferences.sigcomm.org/sigcomm/2013/papers/hotplanet/p27.pdf">Efficient social network data query processing on MapReduce. <i>(2013)</i></a>
<div class="comments">In this paper, a method is provided to translate SPARQL (query language in social network data analysis). Existing methods translate SPARQL queries into a series of SQL joins and then map the SQL join flow to MapReduce jobs. This method is inefficient, so the authors improve the efficiency through using filter of multiple joins to replace SQL join and putting selection part into the part of join for the job. 
<br>In multiple-join-with-filter, the filter key is defined on the fields among the subset of the tables in the join, and this filter key is different from join key. The filter key could filter the entries calculated by traditional multiple join, and this key should be tagged in map phase and reduce will identify the filter key by this tag. In select-join, select group generates the base tables and join group is used to join tables. And in join group, multiple-join-with-filter will be used if possible. In the select group, base table generation will be finished in the map side to reduce the shuffle cost of it. And the experiment using the benchmarks for social network proves the performance improvement by applying this method.
</div></li>
<li class="paper"><a href="https://ece.uwaterloo.ca/~aelgohar/Farahat_CSS_ICDM2013.pdf">Distributed Column Subset Selection on MapReduce. <i>(2013)</i></a>
<div class="comments">In this paper, MapReduce program is applied to solve Colum Subset Selection (CSS) problem. It is used to find out the columns that reflect the important features of the matrix. It applies random projection to get the basic sketch of the matrix and uses a kind of greedy algorithm to finish the selection phase among several matrices.
<br>In random projection step, the algorithm generates the concise representation matrix of the span of the data matrix, and then this relatively small matrix will be distributed to all the machines in the cluster. In generalized CSS, a centralized selection algorithm for greedy generated CSS is applied to perform columns selection from many different submatrices.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1211.6176.pdf">Shark: SQL and rich analytics at scale. <i>(2013)</i></a>
<div class="comments">In this paper, a system Shark is provided to support SQL query and machine learning that can be implemented in SQL. Shark produces a MapReduce execution method to complete SQL queries, and it includes Spark that is the MapReduce computing engine for Shark and resilient distributed datasets (RDDs), the main abstraction of Spark.
<br>Spark supports computation of DAGs, and provides a kind of in-memory storage abstraction RDDs to keep data in memory and automatically reconstructs it after failures. Besides, this engine is optimized to efficiently manage tasks with low latency. And RDDs provides several benefits for large-scale computing setting. RDDs can be written in memory without network transmission and Spark can copy RDD in memory to save replication cost. And lost RDD could be rebuilt in parallel to speed up recovery.
</div></li>
<li class="paper"><a href="http://ceur-ws.org/Vol-1133/paper-02.pdf">Binary Theta-Joins using MapReduce: Efficiency Analysis and Improvements. <i>(2014)</i></a>
<div class="comments">In this paper, a mechanism is designed to better support join operation that is implemented in MapReduce programs. This method applies a matrix to display the workload among different reducers. And it explains how to improve 1-Bucket-Theta and M-Bucket algorithms that implemented in MapReduce programs. The performance of JM (Join Matrix) is evaluated from three metrics: replication rate, maximum reducer input and input imbalance that is the ratio of maximum reducer input to the average reducer input. In addition, this method could obviously decrease the communication cost.
</div></li>
</ul>
<h2>Graph Algorithm in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="https://cs.wmich.edu/gupta/teaching/cs5950/sumII10cloudComputing/graphAlgo%20in%20mapReduce%20paper%20p78-lin.pdf">Design patterns for efficient graph algorithms in MapReduce. <i>(2010)</i></a>
<div class="comments">In this paper, design patterns are presented to process very large graph through MapReduce programming. It provides the programming model for this kind of data processing in MapReduce and proposes three patterns that can be applied in large graph processing to reach a high performance. Besides it conduct experiments of large web graph processing to demonstrate the effect of these patterns.
<br>For large-scale graph processing in MapReduce, the id of the vertex is key and the record of the vertex structure is value. The whole graph is divided into blocks, each block is computed through mapper function, and partial results will be transferred to reducers through shuffle phase, reducers will use reducer function to finish the computation of the graph. To solve the inefficiency of previous graph processing, this paper provides three design patterns.
<br>The first is In-Mapper Combining. Combiner of MapReduce could reduce the data size transferred during shuffle, but it is not specified in MapReduce and combiner does not actually reduce the key-value pairs because combiner is executed after the mapper output is spilled to disk. In-Mapper Combining could solve this problem by moving combiner inside mapper function to eliminate multiple messages generated from map. This design pattern could reduce the cost of materializing temporary key/value pairs using combiner.
<br>The second is Schimmy. The cost in shuffling the graph structure information from map to reduce is not necessary and is too high for large graph. The schimmy design pattern could address the inefficiency, and it is similar to parallel merge join. It uses the same partition function in MapReduce so the number of reducers equals to the number of input. This could eliminate the cost of shuffling graph structure from map to reduce.
<br>The third is Range Partitioning. For graph processing, it is advantageous to partition adjacent vertices in the same block. And web pages in the same domain are more densely connected than pages across different domains. Range Partitioner function splits a graph to ensure pages within a domain in a block.
</div></li>
<li class="paper"><a href="http://islab.kaist.ac.kr/chungcw/InterConfPapers/km0805-ha-myung.pdf">An Efficient MapReduce Algorithm for Counting Triangles in a Very Large Graph. <i>(2013)</i></a>
<div class="comments">In this paper, MapReduce is used to calculate the number of triangles in very large graph can reach high efficiency through eliminating computing redundancy. And it is achieved by applying type classification for different kinds of triangles in the graph. 
<br>This algorithm counts the number of triangles in the graph based on graph partitioning. Previous algorithm’s drawback for graph partitioning is the redundant computation of triangles. The cause of redundant computation is the duplicate edges output from the map, and this will lead to network overload during shuffle phase. Triangle Type Partition (TTP) algorithm of this paper solves the duplication problem in previous algorithm by classifying and processing triangles by three types: three nodes in the same partition, two nodes in one partition and the other in another partition, three nodes are in three different partitions. And it applies the real dataset to prove the performance improvement for such large graphs.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1203.5387.pdf">Finding connected components in map-reduce in logarithmic rounds. <i>(2013)</i></a>
<div class="comments">In this paper, new algorithms are proposed to compute the connected parts in a graph. One algorithm is Hash-to-MIin and the other one is Hash-Greater-to-Min [Rastogi et al. 2013]. And the main contribution of these algorithms is the hash function used in these algorithms. They are demonstrated to reach high performance with only O(logn) cost in MapReduce job execution.
<br>These two algorithms could compute the connected components of the graph in O(logn) rounds and still work efficiently for large-scale social network graph which need to be stored out of memory. Besides, these two algorithms can be extended for linkage clustering, which could also completes in O(logn) rounds. And different datasets are used in the experiments to evaluate two algorithms presented.
</div></li>
</ul>
<h2>Machine Learning in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/pdf/1303.3517.pdf">Iterative mapreduce for large scale machine learning. <i>(2013)</i></a>
<div class="comments">In this paper, it discusses how to design iterative MapReduce for machine learning algorithms. MapReduce has no such support and MapReduce does not recognize the iterative nature in machine learning algorithms. A method is provided to make MapReduce to support iteration for machine learning.
<br>In this programming model, iterative mapredue is a collection of operators that creates dataflow programs. And each operator receives input and produces output. This model contains three key operators: MapReduce, Sequential and Loop. Loop operator is the core element of the extension to basic MapReduce, it is responsible for driving each iteration and producing initial model. MapReduce operator processes training data based on current model and produces aggregate statistic. Then Sequential operator will use the aggregate statistic to update the model and return control to Loop operator for continuing iteration. And iterative mapreduce physical plan is presented.
<br>For optimization, it refers to multiple aspects: data-local scheduling, loop-aware scheduling, caching and efficient data-serialization. And it designs an optimizer to provide optimization chances regarding the number of machines, and evaluates the performance of this optimizer through experiments. This optimizer considers two purposes: to minimize the response time and to minimize the job cost. It contains two choices, one is optimal aggregation tree for intermediate statistics, and the other is optimal partitioning for training data.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1207.0141.pdf">Efficient processing of k nearest neighbor joins using mapreduce. <i>(2012)</i></a>
<div class="comments">In this paper, a method is introduced to apply MapReduce to sovle KNN problem (k nearest neighbor join), which used for data analysis and mining methods such as k-means clustering. The first step of this method is data preprocessing, the second step is the first MapReduce and the third step is second MapReduce. In the preprocessing step, it finds a set of pivots based on the input dataset. In the first mapreduce job that only contains a map phase, these pivots are used to partition the input data after computing the distance between the data and the pivot. Then in the second mapreduce job, mappers find the subsets of the dataset based on the paritioning and reducers will perform the kNN join.In addition, it provides cost model and grouping methods to reduce data replication to decrease the cost in data transmission between map and reduce.
</div></li>
<li class="paper"><a href="http://www.vldb.org/pvldb/vol7/p241-onizuka.pdf">Optimization for iterative queries on MapReduce. <i>(2013)</i></a>
<div class="comments">In this paper, a system OptIQ is designed to improve the performance of iteration queries implemented in MapReduce by eliminating computing redundancy of MapReduce programs during these iterations. The main ideas of it are table decomposition and materialization, which finds the changes of attributes cross iterations and selects and materializes maximum sub-queries; and automatic incrementalization which finds the changes of tuples cross iterations, generates queries incrementally and evaluate these queries.
<br>In the implementation, Hive is integrated with the query optimization of OptIQ. For view materialization, queries of materializing views are selected out and materialized views are stored on distributed file system and can be used again in the next iterations. For the implementation of automatic incrementalization, query compiler will produce plans for incremental query, and these query plans will take the input data from tables and generate output delta tables that will be stored in distributed file system.
</div></li>
</ul>
<h2>Configuration for MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://www.cs.duke.edu/~hero/files/vldb11-job-optimization.pdf">Profiling, what-if analysis, and cost-based optimization of MapReduce programs. <i>(2011)</i></a>
<div class="comments">In this paper, a real optimization system is implemented to optimize MapReduce from the aspect of configuration parameters tuning. And some important components of this system are profiler, what-if engine and cost-based optimizer.
<br>In profiler, it defines a job profile to describe some aspects of dataflow during the job execution at the task or phase level. A job profile consists of four kinds of fields: dataflow fields, cost fields, dataflow statistics fields and cost statistics fields. Job profiler will apply a tracing tool btrace and configure profiling parameters in MapReduce to collect MapReduce profile files during the execution of this job. And these profile files contains the information of all the fields in a job profile.
<br>In What-If engine, it defines a concept of virtual profile. It constructs a cost model for mapreduce job. This cost model could calculate the job execution time with different configuration parameters if given the job feature information. This cost model could be applied in What-If engine to simulate the MapReduce job execution and estimate the job profile, which is called virtual job profile.
<br>In cost-based optimizer, it searches for optimal configuration for current MapReduce job based on the cost model. The objective function of the search algorithm is the MapReduce job execution time predicted based on the cost model. Then it designs a random recursive search algorithm to search for the optimal configuration guided by the objective function.
</div></li>
<li class="paper"><a href="https://www.cs.duke.edu/~hero/files/dataeng13-whatifengine.pdf">A What-if Engine for Cost-based MapReduce Optimization. <i>(2013)</i></a>
<div class="comments">In this paper, What-if Engine is analyzed in details as the core of the optimization tool Starfish to choose optimal configuration parameters for MapReduce job. The foundation of What-If Engine is a performance model that could measure the job execution phases and related running cost. This cost model measures many aspects information of the MapReduce job dataflow related to CPU and I/O cost. And this cost model provides a detailed description of the time cost in each phase of map task and reduce task. By using this engine, it is possible to obtain the MapReduce job sketch and calculate the corresponding job cost such as execution with different kinds of configuration parameters.
</div></li>
<li class="paper"><a href="https://www.cs.duke.edu/~shivnath/papers/socc2010.pdf">Towards automatic optimization of MapReduce programs. <i>(2010)</i></a>
<div class="comments">In this paper, a new approach is introduced to achieve the MapReduce optimization from the aspect of configuration tuning. It provides the detailed analysis of important MapReduce parameters and the relations among them. In addition, some possible optimization approaches are presented as well: getting rid of the tuning knobs, database query-optimizer-style approach, using of dynamic profiling, reacting through late binding, competition-based approaches and hybrid approaches combing other approaches together. And the optimization system starfish designed in the same research group of the author is actually based on the methods of database query-optimizer-style approach.
</div></li>
</ul>
</div>
<div id="footer"></div>
 
</div>
</body>
</html>
