<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CSC8530</title>
<link rel="stylesheet" href="style.css" type="text/css">
</head>
<body>
<div id="content">
<div id="header">
<span id="course"> CSC8530 </span>
<span id="sign">Kewen Wang</span>
</div>
<div id="leftcontent"> 
<a href="index.html"><img id="logo" src="MapReduce.png" alt="LOGO"></a>
<div id="taskslist">Tasks List</div>
<div id="nav">
<ul id="list">
<li class="task"><a class="tasklink" href="index.html">Topic Selection</a></li>
<li class="task"><a class="tasklink" href="paperlist.html">Bibliography of literature found</a></li>
<li class="task"><a class="tasklink" href="comment.html">Annotated Bibliography of the literature found</a></li>
<li class="task"><a class="tasklink" href="result.html">Detailed Annotated Bibliography and Classification of the Results</a></li>
<li class="task"><a class="tasklink" href="survey.html">Survey Paper</a></li>
</ul>
</div>
<div id="contact">Contact</div>
<div id="email">kwang12@student.gsu.edu</div>
</div>
<div id="rightcontent">
<h1>Classification</h1>
<h2>MapReduce Architecture</h2>
<ul id="paperlist">
<li class="paper"><a href="http://idning-ebook.googlecode.com/svn/trunk/google/%23mapreduce-osdi04.pdf">MapReduce: Simplified data processing on large clusters. <i>(2004)</i></a>
<div class="comments">This paper introduces MapReduce programming model from Google Inc. This computation model takes a set of key/value pairs as input and generates a set of key/value pairs as output. The two main functions of this model are map and reduce. Map function processes the input data, and transfer the non-structural input data into key/value pairs. Reduce function merges the output data of map function and produce final output data in the format of key/value pairs. Many interesting programs could be implemented by applying MapReduce: distributed grep, count of URL access frequency, reverse web-link graph, term-vector per host, inverted index and distributed sort.
<br>For MapReduce computation model, it follows master-worker structures for MapReduce job execution. Master machine keeps the status and identity of worker machines, and is responsible for assigning map/reduce tasks to worker machines.
<br>In the distributed environment, the input data of MapReduce job is split into M pieces, and then the copies of MapReduce program is sent to master and worker machines. Master machine will assign M map tasks to workers, and R reduce tasks to other workers. Workers of map tasks read the corresponding input split and generate key/value pairs according map function. These output pairs in the buffer will be periodically written to local disk. The location information of these pairs will be passed to master. Notified by the master, workers of reduce task use remote procedure call to read output of map workers and sort them so that pairs with same key are grouped together. The corresponding intermediate values will be sent to reduce function, and produce final output for this reduce partition. When all the map and reduce tasks are completed, output will return to the user program.
<br>MapReduce model has the fault tolerance mechanism when workers or master fails, and it has locality for data reading through 3 copies of input data. In addition, the master runs a HTTP server and maintains a status pages for running MapReduce job. This status page provides data flow information and job execution time information.
</div></li>
<li class="paper"><a href="http://www.cs.arizona.edu/~bkmoon/papers/sigmodrec11.pdf">Parallel data processing with MapReduce: a survey. <i>(2012)</i></a> 
<div class="comments">This paper is a survey paper about MapReduce. It introduces the architecture of MapRedue programming for parallel data processing. In this paper, it uses Hadoop as an example to compare with DBMS to summarize the advantages and disadvantages of MapReduce. 
<br>It analyzes five advantages of MapReduce. It contains only two main functions map and reduce and physical distribution of the MapReduce is not required, so it is simple and easy to use; it is flexible to process irregular or unstructured data compared to DBMS because it does not depend on data model; MapReduce is independent of the storage layers; it provides fault tolerance to ensure continue work when some working machines fail; MapReduce could support high scalability that Hadoop could scale out to support more than 4000 nodes.
<br>But MapReduce has five disadvantages compared to DBMS. MapReduce does not provide direct support for high-level languages like SQL, which is supported by DBMS; MapReduce has no schema and no index, so it needs to parse each element of the input data; it is a single fixed dataflow, some algorithms are not supported well by using MapReduce; pipeline parallelism is not exploited in MapReduce, and MapReduce does not have execution plans which are often optimized in DBMS to reduce data transmission; MapReduce is too young and not mature compared to 40 years old DBMS, and few third-party tools are available to support. Moreover, this paper presents some approaches to improve the MapReduce framework: some tools like Pig and Hive could provide some high-level languages like SQL in MapReduce; data compression could solve data size problem; mapreduce model could be modified to support iteration programs or pipelined processing; using index structures to reduce I/O cost; schedule scheme could be carefully designed to improve execution time; tuning system parameters to improve MapReduce job performance.
</div></li>
</ul>
<h2>Optimization Approaches</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/pdf/1302.2966.pdf">The Family of MapReduce and Large Scale Data Processing Systems. <i>(2013)</i></a>
<div class="comments">This paper describes the framework of MapReduce, and provides some techniques that can improve the performance and capabilities of MapReduce from different perspectives and covers various large scale data processing systems based on idea of MapReduce, similar to MapReduce or implemented on the top of MapReduce framework.
<br>These possible improvements are based on the limitations of the basic MapReduce architecture. One main limitation of MapReduce is that it does not well support joining of multiple datasets, an approach to optimize the communication cost of join operation could improve corresponding mapreduce programs; some systems like HaLoop could support iterative processing for MapReduce in some algorithms; data and process sharing in MRShare could reduce data transmission cost in MapReduce; another approach is to add data index to reduce query cost; systems like CoHadoop provide mechanism to control data placement to achieve good load balance for MapReduce job; systems like Incoop supports incremental computations to support pipeling and streaming operations; and analyzing MapReduce programs to detect opportunities for relational style optimizations. 
<br>MapReduce has no SQL-like support, some efforts have been proposed to provide SQL interfaces on the top of MapReduce. Sawzall is script language on top of MapReduce; Pig Latin provides SQL query model above MapReduce programs; Hive supports queries in SQL; Tenzing is SQL query execution engine; SQL/MR allows custom-defined functions in any programming language and improve SQL functionality; HadoopDB is a hybrid system to combine scalability of MapReduce and efficiency of parallel databases; and Jaql is a kind of query language designed for Javascript Object Notation (JSON).
<br>Moreover, this paper introduces some systems that are similar to basic ideas of MapReduce framework: SCOPE, Dryad/DryadLinq, Spark, Nephle/Pact, Boom Analytics, Hyracks/ ASTERIX.
</div></li>
<li class="paper"><a href="https://lambda.uta.edu/mrql.pdf">An optimization framework for map-reduce queries. <i>(2012)</i></a>
<div class="comments">This paper presents a new MapReduce-like query language for map-reduce computations: MRQL (Map-Reduce Query Language). It designs and implements an effective optimization framework for Map-Reduce queries. This paper provides details about how to translate MRQL queries to efficient execution of MapReduce jobs. 
<br>MRQL can support input data of JSON, XML, binary and record text documents, and its semantics has been inspired by the functional programings. This framework contains some MR operators that capture the MRQL functionality. The MRQL will be firstly translated into algebraic form and then translate to a physical plan of MR operators.
<br>Besides, this paper presents an optimization framework to provide a good plan of physical operators. It includes 5 steps: simplifing the query by eliminating some query nesting, constructing the query graph of the execution plan, deriving an algebraic form from the query graph, algebraic optimization to minimize the number of MR jobs and to avoid redundant operations, synthesizing the combine function.
</div></li>
<li class="paper"><a href="http://www.infosun.fim.uni-passau.de/cl/publications/docs/DAL13mapreduce.pdf">Modeling and Optimizing MapReduce Programs. <i>(2013)</i></a>
<div class="comments">It is a technical report about MapReduce optimization. It represents MapReduce programming model from the aspect of functional model, and analyzes main steps of MapReduce execution: Mapper, shuffle and Reducer. 
<br>It uses functional language Haskell to represent the functional model of MapReduce, and model the data flow by means of steps of Mapper and Reducer tasks. Mapper tasks are executed on many cluster nodes, and user-defined mapper function is run in distributed. Shuffle execution is in the middle between mapper and reducer. Reducer is described in the flow of sort, group and reducer.
<br>Besides, it constructs a cost model to analyze the time cost of MapReduce programs. It constructs the equations to calculate the time cost of a MapReduce job execution based on the input data information and parameters. Based on the MapReduce functional model and cost model, it provides rules for optimization and demonstrates the effectiveness.
</div></li>
<li class="paper"><a href="http://cloud.pubs.dbs.uni-leipzig.de/sites/cloud.pubs.dbs.uni-leipzig.de/files/Jiang2010TheperformanceofmapreduceAnindepthstudy.pdf">The Performance of MapReduce: An In-depth Study. <i>(2010)</i></a>
<div class="comments">This paper is about the research of MapReduce performance. It discusses some factors that affect the performance of MapReduce. MapReduce programming model is an important factor that impacts its performance. And MapReduce is independent of storage, there are three factors: I/O mode, the way of fetching data from storage system; data parsing, the scheme how reader parse the format of the record; indexing, the methods to utilize indexes to speed up data processing; scheduling, the scheme to assign map and reduce tasks execution through scheduler.
<br>And these factors could be combined to improve the MapReduce performance, this paper applies experiments on benchmark of different kinds of tasks to demonstrate the performance improvement by tuning these factors.
</div></li>
</ul>
<h2>Improved MapReduce System</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/ftp/arxiv/papers/1104/1104.3217.pdf">Automatic optimization for MapReduce programs. <i>(2011)</i></a>
<div class="comments">This paper describes a system MANIMAL to automatically optimize MapReduce programs. It is a data-aware optimization method, which does not change the MapReduce programs. 
<br>MANIMAL targets three kinds of optimization: selections that could be optimized by using B+ tree to scan only the relevant faction of the input data; projection that optimized by storing only bytes necessary for executing the MapReduce codes, this optimization is similar to column-store; data compression could reduce data size in storing and transferring during MapReduce execution, MANIMAL enables delta-compression and direct-compression.
<br>This system MANIMAL consists of three components: analyzer (examines MapReduce programs and sends optimization descriptor to optimizer), optimizer (choose optimized execution plan sent to execution fabric) and execution fabric (execution the plan as MapReduce). The analyzer will examine the MapReduce programs to find optimization opportunities, so it is the foundation of MANIMAL system. Analyzer applies control flow graph (CFG) to analyze all possible paths the program may take, and uses dataflow analysis to compute reaching definitions.
</div></li>
<li class="paper"><a href="http://static.usenix.org/events/nsdi10/tech/full_papers/condie.pdf">MapReduce Online. <i>(2010)</i></a>
<div class="comments">This paper develops a pipelining version of Hadoop: Hadoop Online Prototype (HOP). HOP extends Hadoop to support pipelining between tasks and between jobs. 
<br>Hadoop Online supports pipelining within a job through modifying the mapreduce codes to enable mapper pushing data to reducers instead of pulling data by reducers, and pipeline stalls could free map tasks after completing the tasks. And it could pipeline the reduce output of one job directly to the map tasks of the next job to support pipelining between jobs. Besides, it supports single-job online aggregation and multi-job online aggregation.
<br>Moreover, the pipelined Hadoop allows MapReduce jobs run continuously accepting new data when it is available and analyzing it immediately. For map tasks, output data is already sent to reduce tasks once it is generated; for reduce tasks, reduce function need to be invoked periodically when map output available at the reducers.
</div></li>
<li class="paper"><a href="http://pdf.aminer.org/000/225/039/a_practical_approach_to_static_node_positioning.pdf">SQL/MapReduce: A practical approach to self-describing, polymorphic, and parallelizable user-defined functions. <i>(2009)</i></a>
<div class="comments">This paper presents SQL/MapReduce (SQL/MR), a new framework for User-Defined Functions (UDFs). It is parallel designed to process parallel computation in massively-parallel relational database. 
<br>Query syntax of SQL/MR is similar to SQL query. It requires ON clause to specify the input to the SQL/MR function, and the PARTITION BY clause is used to partition the input. And ORDER BY clause will be used to sort the data. Besides, user can add some custom argument clauses. The execution model provided by SQL/MR functions is a generalization of MapReduce. It contains programming interfaces: runtime contract, processing data functions, combiner functions and running aggregates. SQL/MR is integrated in nCluster, which is a shared-nothing parallel database. The implementation of SQL/MR in nCluster needs define the interaction of SQL/MR function with query planning and query execution framework.
</div></li>
<li class="paper"><a href="http://pasa-bigdata.nju.edu.cn/people/ronggu/pub/SHadoop_JPDC.pdf">SHadoop: Improving MapReduce performance by optimizing job execution mechanism in Hadoop clusters. <i>(2014)</i></a>
<div class="comments">This paper presents an optimized version of Hadoop: SHadoop, to improve the execution performance of MapReduce jobs. SHadoop optimizes setup/cleanup tasks in MapReduce job to shorten the startup and cleanup time and optimizes job/tasks execution event notification mechanism to provide an instant messaging communication mechanism for efficient critical event notification.
<br>In MapReduce job setup/cleanup, SHadoop execute the job setup and cleanup task on the JobTracker side instead of sending messages to TaskTracker to launch the job setup and cleanup task by periodical heartbeats in original Hadoop. It will avoid four heartbeats in the standard Hadoop.
<br>For notification mechanism, SHadoop separates the critical event messages from heartbeat messages and design a new instant messaging communication mechanism for critical message notification so critical event (like task completion event) message will be sent to the JobTracker immediately.
</div></li>
</ul>
<h2>SQL in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-7.pdf">Ysmart: Yet another sql-to-mapreduce translator. <i>(2011)</i></a>
<div class="comments">This paper presents a system YSmart that is built on the top of Hadoop to translate SQL query into MapReduce programs. YSmart is designed as a translator with specific considerations of intra-query correlations to improve execution efficiency.
<br>Auto-generated MapReduce may have too long time execution time in some critical operations, and may create unnecessary jobs. The bottleneck of translating SQL to MapReduce is that they cannot solve the limitations of MapReduce structure for complex queries and they cannot utilize the opportunities in intra-query of complex query.
<br>YSmart is designed to optimize complex query without changing MapReduce framework. It firstly detects intra-query correlations in a query, and then use set of rules to generate optimized MapReduce jobs through Common MapReduce Framework (CMF). YSmart aims at minimal number of jobs to execute many correlated operations in the query. For selection and projection operation, it is straightforward. For aggregation with grouping, it can be finished in the reduce phase. For joining of two datasets, each data set is partitioned by its columns and join is finished in reduce phase.
<br>SQL to MapReduce translators use one-operation-to-one-job mode used by DBMSs. For DBMS, it uses a pipelined and iterator-based interconnection among many operators. But MapReduce is different from DBMS, so one-operation-to-one-job mode is not proper, and another mode many-to-one of executing many operations in a single job would be a more efficient choice than one-to-one translation.
<br>YSmart detect three correlations: input correlation, transit correlation and job flow correlation. For input correction, the corresponding jobs can share the table scan in map phase; for transit correlation, there exists overlapped data between map outputs; for job flow correlation, the node can be evaluated in the reduce phase.
<br>Common MapReduce Framework (CMF) is the foundation of YSmart, it provides flexible framework to allow different types of MapReduce jobs and merges multiple jobs in a common job with low cost. The common mapper executes selection or projection operations and common reducer executes join or aggreration operations, and post-job computation could execute further computation for the output of previous jobs.
<br>And experiments are applied to demonstrate its performance over Hive and Pig, two widely-used translators from SQL to MapReduce.
</div></li>
<li class="paper"><a href="http://www.chinacloud.cn/upload/2013-10/13100807567165.pdf">JackHare: a framework for SQL to NoSQL translation using MapReduce. <i>(2013)</i></a>
<div class="comments">This paper proposes a framework JackHare to translate SQL to MapReduce phases for processing the unstructured data in HBase. JackHare framework includes SQL query compiler and uses JDBC driver to process unstructured data in HBase as datastore.
<br>The process of operations in JackHare is: submit ANSI-SQL query through SQL client application, and compiler scans and parses the query, look up the table of HBase and generate MapReduce programs according to query commands, access HBase and execute MapReduce job, the results will be returned to SQL client application based on RDB schema.
<br>Then authors analyze the methods about how to translate basic or extended SQL statements and the combination of SQL clauses to an execution plan of MapReduce phases. For select statement, map phase is sufficient to get query result and reduce is not necessary; for where clause, filter instance can be customized to filter input table; for grouping query, the mappers select the value of the column specified by the query as the key, and the reducers collect the key/value pairs with the same key; for aggregate query, the aggregation function is implemented in the reduce phase; for joining query, another job may be needed; for sorting query, the rows of the table are sorted by the row key in HBase.
</div></li>
<li class="paper"><a href="http://conferences.sigcomm.org/sigcomm/2013/papers/hotplanet/p27.pdf">Efficient social network data query processing on MapReduce. <i>(2013)</i></a>
<div class="comments">This paper presents an efficient method to translate SPARQL (a query language for processing social network data). Existing methods translate SPARQL queries into a series of SQL joins and then map the SQL join flow to MapReduce jobs. This method is inefficient, so the authors improve the efficiency through two primitives: using multiple-join-with-filter to substitute SQL join and merging the selection stage’s job into the join stage’s job. 
<br>In multiple-join-with-filter, the filter key is defined on the fields among the subset of the tables in the join, and this filter key is different from join key. The filter key could filter the entries calculated by traditional multiple join, and this key should be tagged in map phase and reduce will identify the filter key by this tag. In select-join, select group generates the base tables and join group is used to join tables. And in join group, multiple-join-with-filter will be used if possible. In the select group, base table generation will be finished in the map side to reduce the shuffle cost of it. 
<br>And the experiment on social network benchmarks demonstrates this method’s 2x speedup compared to the traditional method.
</div></li>
<li class="paper"><a href="https://ece.uwaterloo.ca/~aelgohar/Farahat_CSS_ICDM2013.pdf">Distributed Column Subset Selection on MapReduce. <i>(2013)</i></a>
<div class="comments">This paper proposes a MapReduce program to solve Colum Subset Selection (CSS) problem that is defined as the selection of the most representative columns of a data matrix. This algorithm firstly learns a concise representation of the data matrix by using random projection and then applies a centralized greedy algorithm for CSS to perform the selection from different sub-matrices.
<br>In random projection step, the algorithm generates the concise representation matrix of the span of the data matrix, and then this relatively small matrix will be distributed to all the machines in the cluster. In generalized CSS, a centralized selection algorithm for greedy generated CSS is applied to perform columns selection from many different submatrices.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1211.6176.pdf">Shark: SQL and rich analytics at scale. <i>(2013)</i></a>
<div class="comments">This paper designs a data analysis system Shark to provide the support for efficient SQL query processing and sophisticated machine learning by using SQL. Shark generalizes a MapReduce-like runtime to run SQL effectively, it includes Spark that is the MapReduce-like cluster computing engine of Shark and resilient distributed datasets (RDDs), the main abstraction of Spark.
<br>Spark supports computation of DAGs, and provides a kind of in-memory storage abstraction RDDs to keep data in memory and automatically reconstructs it after failures. Besides, this engine is optimized to efficiently manage tasks with low latency. And RDDs provides several benefits for large-scale computing setting. RDDs can be written in memory without network transmission and Spark can copy RDD in memory to save replication cost. And lost RDD could be rebuilt in parallel to speed up recovery.
</div></li>
<li class="paper"><a href="http://ceur-ws.org/Vol-1133/paper-02.pdf">Binary Theta-Joins using MapReduce: Efficiency Analysis and Improvements. <i>(2014)</i></a>
<div class="comments">This paper proposes the methods to improve the efficiency of binary theta-joins using MapReduce. It uses a join matrix (JM) to represent the workload partitioned across the reducers. This paper discusses how to improve the algorithms 1-Bucket-Theta and M-Bucket, which are the algorithms for theta-joins in MapReduce. The performance of JM is evaluated from three metrics: replication rate, maximum reducer input and input imbalance that is the ratio of maximum reducer input to the average reducer input. Proposal of the paper could reduce the communication cost and lead to maximum reducer input.
</div></li>
</ul>
<h2>Graph Algorithm in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="https://cs.wmich.edu/gupta/teaching/cs5950/sumII10cloudComputing/graphAlgo%20in%20mapReduce%20paper%20p78-lin.pdf">Design patterns for efficient graph algorithms in MapReduce. <i>(2010)</i></a>
<div class="comments">This paper presents design patterns to implement large-scale graph processing in MapReduce. Firstly, it introduces the MapReduce programming model and class of graph algorithms need to implement in MapReduce. Then it provides three new design patterns to improve the efficiency in the previous implementation, and uses a web graph with 1.4 billion links to demonstrate the improvement of the proposed design patterns.
<br>For large-scale graph processing in MapReduce, the id of the vertex is key and the record of the vertex structure is value. The whole graph is divided into blocks, each block is computed through mapper function, and partial results will be transferred to reducers through shuffle phase, reducers will use reducer function to finish the computation of the graph. To solve the inefficiency of previous graph processing, this paper provides three design patterns.
<br>The first is In-Mapper Combining. Combiner of MapReduce could reduce the data size transferred during shuffle, but it is not specified in MapReduce and combiner does not actually reduce the key-value pairs because combiner is executed after the mapper output is spilled to disk. In-Mapper Combining could solve this problem by moving combiner inside mapper function to eliminate multiple messages generated from map. This design pattern could reduce the cost of materializing temporary key/value pairs using combiner.
<br>The second is Schimmy. The cost in shuffling the graph structure information from map to reduce is not necessary and is too high for large graph. The schimmy design pattern could address the inefficiency, and it is similar to parallel merge join. It uses the same partition function in MapReduce so the number of reducers equals to the number of input. This could eliminate the cost of shuffling graph structure from map to reduce.
<br>The third is Range Partitioning. For graph processing, it is advantageous to partition adjacent vertices in the same block. And web pages in the same domain are more densely connected than pages across different domains. RangePartitioner function splits a graph to ensure pages within a domain in a block.
</div></li>
<li class="paper"><a href="http://islab.kaist.ac.kr/chungcw/InterConfPapers/km0805-ha-myung.pdf">An Efficient MapReduce Algorithm for Counting Triangles in a Very Large Graph. <i>(2013)</i></a>
<div class="comments">This paper provides an efficient MapReduce algorithm to count the triangles in a large graph by solving the problem of redundant computation of triangles in previous algorithms. This new algorithm adds the step of triangle type classification to avoid this drawback. 
<br>This algorithm counts the number of triangles in the graph based on graph partitioning. Previous algorithm’s drawback for graph partitioning is the redundant computation of triangles. The cause of redundant computation is the duplicate edges output from the map, and this will lead to network overload during shuffle phase. Triangle Type Partition (TTP) algorithm of this paper solves the duplication problem in previous algorithm by classifying and processing triangles by three types: three nodes in the same partition, two nodes in one partition and the other in another partition, three nodes are in three different partitions.
<br>Moreover, this paper uses real-world dataset to demonstrate this new algorithm’s performance in large and dense graph.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1203.5387.pdf">Finding connected components in map-reduce in logarithmic rounds. <i>(2013)</i></a>
<div class="comments">This paper proposes two new algorithms for connected components computation in large graph: Hash-to-MIin algorithm and Hash-Greater-to-Min algorithm. These two algorithms apply hashing strategies to achieve logarithmic rounds and less communication cost. 
<br>These two algorithms could compute the connected components of the graph in O(logn) rounds and still work efficiently for large-scale social network graph which need to be stored out of memory. Besides, these two algorithms can be extended for linkage clustering, which could also completes in O(logn) rounds.
<br>In this paper, authors use a variety of real datasets to evaluate the performance of these two algorithms.
</div></li>
</ul>
<h2>Machine Learning in MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://arxiv.org/pdf/1303.3517.pdf">Iterative mapreduce for large scale machine learning. <i>(2013)</i></a>
<div class="comments">This paper provides a method to make MapReduce to support iteration for machine learning. For machine learning, MapReduce does not recognize the iterative nature in machine learning algorithms. In this paper, authors explain an iterative MapReduce programming model for machine learning. 
<br>In this programming model, iterative mapredue is a collection of operators that creates dataflow programs. And each operator receives input and produces output. This model contains three key operators: MapReduce, Sequential and Loop. Loop operator is the core element of the extension to basic MapReduce, it is responsible for driving each iteration and producing initial model. MapReduce operator processes training data based on current model and produces aggregate statistic. Then Sequential operator will use the aggregate statistic to update the model and return control to Loop operator for continuing iteration. And iterative mapreduce physical plan is presented.
<br>For optimization, it refers to multiple aspects: data-local scheduling, loop-aware scheduling, caching and efficient data-serialization. And the paper presents an optimizer to provide theoretically optimal choices for the fan-in f and machines number N, and evaluates the optimizer by experiments. This optimizer considers two purposes: to minimize the response time and to minimize the job cost. It contains two choices, one is optimal aggregation tree for intermediate statistics, and the other is optimal partitioning for training data.
</div></li>
<li class="paper"><a href="http://arxiv.org/pdf/1207.0141.pdf">Efficient processing of k nearest neighbor joins using mapreduce. <i>(2012)</i></a>
<div class="comments">This paper introduces a method to apply MapReduce to process k nearest neighbor join (kNN), which is widely used in data mining and analysis applications like k-means clustering.     This method consists of three main steps: data preprocessing, first MapReduce and second MapReduce. In the preprocessing step, it finds a set of pivots based on the input dataset. In the first mapreduce job that only contains a map phase, these pivots are used to partition the input data after computing the distance between the data and the pivot. Then in the second mapreduce job, mappers find the subsets of the dataset based on the paritioning and reducers will perform the kNN join.
<br>Moreover, this paper presents a cost model and two grouping strategies to minimize the replicas of data in order to reduce the shuffling cost between Map and Reduce.
</div></li>
<li class="paper"><a href="http://www.vldb.org/pvldb/vol7/p241-onizuka.pdf">Optimization for iterative queries on MapReduce. <i>(2013)</i></a>
<div class="comments">This paper describes a system OptIQ to optimize iterative queries on MapReduce, OptIQ removes redundant computations among different iterations. It involves two ideas: table decomposition & materialization, which finds the changes of attributes cross iterations and selects and materializes maximum sub-queries; and automatic incrementalization which finds the changes of tuples cross iterations, generates queries incrementally and evaluate these queries.
<br>In the implementation, Hive is integrated with the query optimization of OptIQ. For view materialization, queries of materializing views are selected out and materialized views are stored on distributed file system and can be used again in the next iterations. For the implementation of automatic incrementalization, query compiler will produce plans for incremental query, and these query plans will take the input data from tables and generate output delta tables that will be stored in distributed file system.
</div></li>
</ul>
<h2>Configuration for MapReduce</h2>
<ul id="paperlist">
<li class="paper"><a href="http://www.cs.duke.edu/~hero/files/vldb11-job-optimization.pdf">Profiling, what-if analysis, and cost-based optimization of MapReduce programs. <i>(2011)</i></a>
<div class="comments">This paper introduces a method to optimize MapReduce program by tunning MapReduce parameter configurations. In this paper, authors describe three main components of this method: profiler, what-if engine and cost-based optimizer.
<br>In profiler, it defines a job profile to describe some aspects of dataflow during the job execution at the task or phase level. A job profile consists of four kinds of fields: dataflow fields, cost fields, dataflow statistics fields and cost statistics fields. Job profiler will apply a tracing tool btrace and configure profiling parameters in MapReduce to collect MapReduce profile files during the execution of this job. And these profile files contains the information of all the fields in a job profile.
<br>In What-If engine, it defines a concept of virtual profile. It constructs a cost model for mapreduce job. This cost model could calculate the job execution time with different configuration parameters if given the job feature information. This cost model could be applied in What-If engine to simulate the MapReduce job execution and estimate the job profile, which is called virtual job profile.
<br>In cost-based optimizer, it searches for optimal configuration for current MapReduce job based on the cost model. The objective function of the search algorithm is the estimated job execution time based on the cost model. Then it designs a random recursive search algorithm to search for the optimal configuration guided by the objective function.
</div></li>
<li class="paper"><a href="https://www.cs.duke.edu/~hero/files/dataeng13-whatifengine.pdf">A What-if Engine for Cost-based MapReduce Optimization. <i>(2013)</i></a>
<div class="comments">This paper describes What-if Engine, a critical component of Starfish, a tool for optimizing MapReduce by tuning configurations of MapReduce job. Actually, What-if Engine is used to estimate MapReduce job execution time (cost) with chosen MapReduce parameter configuration. 
<br>In this paper, What-if Engine provides an analytical model to estimate dataflow and cost for MapReduce job. And the important part of What-If Engine is the cost model to describe the dataflow of MapReduce job. This cost model measures many aspects information of the MapReduce job dataflow related to CPU and I/O cost. And this cost model provides a detailed description of the time cost in each phase of map task and reduce task.
</div></li>
<li class="paper"><a href="https://www.cs.duke.edu/~shivnath/papers/socc2010.pdf">Towards automatic optimization of MapReduce programs. <i>(2010)</i></a>
<div class="comments">In this paper, author provides a new idea to optimize the MapReduce job performance by tuning MapReduce parameters. This paper analyzes some MapReduce parameters’ impact on the MapReduce job and the interactions among parameters. Moreover, author proposes some potential approaches of tuning parameter configuration: getting rid of the tuning knobs, database query-optimizer-style approach, using of dynamic profiling, reacting through late binding, competition-based approaches and hybrid approaches combing other approaches together. And the optimization starfish designed in the same research group of the author is actually based on the methods of database query-optimizer-style approach.
</div></li>
</ul>
</div>
<div id="footer"></div>

</div>
</body>
</html>
